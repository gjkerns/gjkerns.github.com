---
layout: postsmath
title: Estimating a normal mean with a cauchy prior
excerpt: This is an example (with a Bayesian spin) illustrating one way to compute a complicated expectation (that is, an integral) by the Monte Carlo method. 
categories: R
---


<div id="outline-container-1" class="outline-2">
<h2 id="sec-1">The setup</h2>
<div class="outline-text-2" id="text-1">

<p>When doing statistics the Bayesian way, we are sometimes bombarded with complicated integrals that do not lend themselves to closed-form solutions.  This used to be a problem.  Nowadays, not so much.  This post illustrates how a person can use the Monte Carlo method (and R) to get a good estimate for an integral that might otherwise look unwieldy at first glance.  Of course, in this example, the integral isn't very complicated.  But the <i>method</i> works the same, regardless of the mess in which we find ourselves.  The current example is derived from one in <a href="http://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-21239-5">Monte Carlo Statistical Methods</a> by Robert/Casella (in Chapter 4).  For that matter, check out their <a href="http://www.springer.com/statistics/computanional+statistics/book/978-1-4419-1575-7">Introducing Monte Carlo Methods with R</a>.
</p>

<p>
Suppose we have one observation \( X \sim N(\theta,1) \) but we have a (robust) prior distribution on \(\theta\), namely, \( \theta \sim \mathrm{Cauchy}(0,1) \).  We would like to update our beliefs about \(\theta\) based on the information provided by \(x\).  So our likelihood is 
  \[
  f(x|\theta) = \frac{1}{\sqrt{2\pi}}\exp \left[-\frac{1}{2}(x - \theta)^2  \right],
  \]
  and our prior is
  \[
  g(\theta) = \frac{1}{\pi}\frac{1}{(1 + \theta^{2})}.
  \]
  The posterior distribution is proportional to the likelihood times prior, that is, 
  \[
  g(\theta|x) \propto \exp \left[-\frac{1}{2}(x - \theta)^2  \right] \frac{1}{(1 + \theta^{2})},
  \]
  with the proportionality constant being the reciprocal of
  \[
  C = \int \exp \left[-\frac{1}{2}(x - \theta)^2  \right] \frac{1}{(1 + \theta^{2})} \mathrm{d} \theta.
  \]
  Our point estimate (or best guess) for \(\theta\) will be just the <i>posterior mean</i>, given by
  \[
  E (\theta | \mbox{data}) = \frac{ \int \theta \exp \left[-\frac{1}{2}(x - \theta)^2  \right] \frac{1}{(1 + \theta^{2})} \mathrm{d} \theta }{C}.
  \]

We notice that the integrand for \(C\) looks like <i>something</i> times a Cauchy PDF, where the <i>something</i> (let's call it \(h\)) is
\[
h(\theta) = \pi \exp \left[-\frac{1}{2}(x - \theta)^2  \right],
\] 
so one way to use the Monte Carlo method follows. 
</p><dl>
<dt>Procedure:</dt><dd>Given observed data \(X=x\),
<ol>
<li>Simulate a bunch of Cauchys, \(\theta_{1},\theta_{2},\ldots,\theta_{m}\), say.
</li>
<li>Estimate the integral in the denominator with
     \[
     \frac{1}{m}\sum_{i=1}^{m} \pi\exp\left[-\frac{1}{2}(x - \theta_{i})^2  \right]. 
     \]
</li>
<li>Estimate the integral in the numerator with
     \[
     \frac{1}{m}\sum_{i=1}^{m} \pi\theta_{i} \exp \left[-\frac{1}{2}(x - \theta_{i})^2  \right].
     \]
</li>
<li>Take the ratio, and we're done.
</li>
</ol>

</dd>
</dl>


<p>
The <a href="http://en.wikipedia.org/wiki/Strong_Law_of_Small_Numbers">Strong Law of Large Numbers</a> says that the averages in 2 and 3 both converge to where they should, so the ratio should converge to the right place as well.  
</p>
</div>

</div>

<div id="outline-container-2" class="outline-2">
<h2 id="sec-2">How to do it with R</h2>
<div class="outline-text-2" id="text-2">

<p>The following is an R script which does the above.  For laughs, let's suppose that we observed \(X=3\).
</p>



<pre class="src src-R"><span style="color: #b22222;"># </span><span style="color: #b22222;">cauchyprior.R</span>
set.seed(1)           <span style="color: #b22222;"># </span><span style="color: #b22222;">makes the experiment reproducible</span>
m <span style="color: #008b8b;">&lt;-</span> 2000             <span style="color: #b22222;"># </span><span style="color: #b22222;">number of simulated values</span>
x <span style="color: #008b8b;">&lt;-</span> 3                <span style="color: #b22222;"># </span><span style="color: #b22222;">observed data</span>

<span style="color: #b22222;"># </span><span style="color: #b22222;">Now simulate some random variables</span>
theta <span style="color: #008b8b;">&lt;-</span> rcauchy(m)                  <span style="color: #b22222;"># </span><span style="color: #b22222;">simulate m standard Cauchys</span>
h <span style="color: #008b8b;">&lt;-</span> pi * exp(-0.5*(x - theta)^2)    <span style="color: #b22222;"># </span><span style="color: #b22222;">who wants to write this over and over</span>

Constant <span style="color: #008b8b;">&lt;-</span> mean(h)                  <span style="color: #b22222;"># </span><span style="color: #b22222;">estimate normalizing constant</span>
post.mean <span style="color: #008b8b;">&lt;-</span> mean(theta * h)/mean(h)   <span style="color: #b22222;"># </span><span style="color: #b22222;">estimate posterior mean</span>
</pre>



</div>

</div>

<div id="outline-container-3" class="outline-2">
<h2 id="sec-3">At the command prompt</h2>
<div class="outline-text-2" id="text-3">

<p>After copy-pasting the above into an R session we can see what the results were with an additional
</p>


<pre class="src src-R">Constant
post.mean
</pre>



<p>
For this simple example we can actually calculate what the true values are (to machine precision) with the following: for the constant \(C\) we get
</p>


<pre class="src src-R"><span style="color: #0000ff;">f</span> <span style="color: #008b8b;">&lt;-</span> <span style="color: #a020f0;">function</span>(x) exp(-0.5*(x - 3)^2)/(1 + x^2)  
integrate(f, -<span style="color: #228b22;">Inf</span>, <span style="color: #228b22;">Inf</span>)                    
</pre>



<p>
so our estimate of \(C\) overshot the mark by about 0.03, and in the posterior mean case we get
</p>


<pre class="src src-R"><span style="color: #0000ff;">g</span> <span style="color: #008b8b;">&lt;-</span> <span style="color: #a020f0;">function</span>(x) x * f(x)  
integrate(g, -<span style="color: #228b22;">Inf</span>, <span style="color: #228b22;">Inf</span>)$value / integrate(f, -<span style="color: #228b22;">Inf</span>, <span style="color: #228b22;">Inf</span>)$value
</pre>



<p>
so our estimate of the posterior mean was around 0.05 too high.  If we would like to get better estimates, we could increase the value of <code>m = 2000</code> to something higher (assuming these things are actually converging someplace).
</p>
</div>

</div>

<div id="outline-container-4" class="outline-2">
<h2 id="sec-4">Are we waiting long enough?</h2>
<div class="outline-text-2" id="text-4">

<p>Our estimates were a little bit off; we might like to take a look at a plot to see how we're doing &ndash; is this thing really converging like we'd expect?  We can look at a running average plot to assess convergence. If the plot bounces around indeterminably, that's bad, but if it settles down to a finite constant, that's better.  Here's a quick way to check this out with base graphics.
</p>



<pre class="src src-R">rc <span style="color: #008b8b;">&lt;-</span> cumsum(h)/seq_along(h)        <span style="color: #b22222;"># </span><span style="color: #b22222;">running mean of C</span>
rpm <span style="color: #008b8b;">&lt;-</span> cumsum(h * theta)/cumsum(h)  <span style="color: #b22222;"># </span><span style="color: #b22222;">running posterior mean</span>
</pre>


<p>
Now we plot the results.
</p>



<pre class="src src-R">A <span style="color: #008b8b;">&lt;-</span> data.frame(iter = 1:m, rc = rc, rpm=rpm)
<span style="color: #008b8b;">library</span>(reshape)
A.short <span style="color: #008b8b;">&lt;-</span> melt(A[3:200, ], id=<span style="color: #8b2252;">"iter"</span>)
a <span style="color: #008b8b;">&lt;-</span> ggplot(A.short, aes(iter, value, colour=variable)) + geom_line() +
      opts(title = <span style="color: #8b2252;">"First 200"</span>)
A.long <span style="color: #008b8b;">&lt;-</span> melt(A, id=<span style="color: #8b2252;">"iter"</span>)
b <span style="color: #008b8b;">&lt;-</span> ggplot(A.long, aes(iter, value, colour=variable)) + geom_line() +
      opts(title = <span style="color: #8b2252;">"All 2000 iterations"</span>)
grid.newpage()
pushViewport(viewport(layout = grid.layout(1, 2, widths = unit(c(3,5),<span style="color: #8b2252;">"null"</span>))))
<span style="color: #0000ff;">vplayout</span> <span style="color: #008b8b;">&lt;-</span> <span style="color: #a020f0;">function</span>(x, y)
viewport(layout.pos.row = x, layout.pos.col = y)
print(a, vp = vplayout(1, 1))
print(b, vp = vplayout(1, 2))
</pre>



<div id="fig-yplot" class="figure">
<p><img src="../images/110824.png"  alt="../images/110824.png" /></p>
<p>Running averages for assessing convergence of the estimators</p>
</div>

<p>
In this example, the estimates look to be still unstable at around <code>m = 200</code>, but by the time we reach <code>m = 2000</code> they look to have pretty much settled down.  Here we knew what the true values were, so we could tell immediately how well we were doing.  On the battlefield we are not so lucky.  In general, with Monte Carlo estimates like these it is wise to take a look at some plots to judge the behavior of our estimators.  If our plot looks more like the one on the left, then we should consider increasing the sample size.  If our plot looks more like the one on the right, then maybe we would be satisfied with "close enough". (We can always wait longer, tight purse-strings notwithstanding.)
</p>

</div>

</div>

<div id="outline-container-5" class="outline-2">
<h2 id="sec-5">Other approaches</h2>
<div class="outline-text-2" id="text-5">

<p>When we were looking to estimate \(C\) we noticed that the integrand was <i>something</i> times a Cauchy distribution.  If we look again, we can see that the same integrand also looks like a <i>normal</i> distribution times <i>something</i>. So, another approach would be to simulate a bunch of normals and average the new <i>somethings</i>.  Do we get the same answer (in the limit)?
</p>
<p>
Yes, of course. It turns out, the approach simulating normals does a little bit better than the one simulating Cauchys, but they're really pretty close.  Check out chapter 4 of <a href="http://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-21239-5">Monte Carlo Statistical Methods</a> for discussion on this.
</p>
</div>

</div>

<div id="outline-container-6" class="outline-2">
<h2 id="sec-6">Where to find more&hellip;</h2>
<div class="outline-text-2" id="text-6">


<p>
The above is a variant of an example we did in <a href="https://github.com/gjkerns/STAT5840">STAT 5840, Statistical Computing</a>.  The entire course is available online at <a href="https://github.com/gjkerns/STAT5840">github</a>. Go to the Downloads for a <code>.zip</code> file or <code>.tar.gz</code>.  Or, if you have <a href="http://git-scm.com/">git</a> installed, you can get (git?) it all with
</p><pre class="example">
 git clone git://github.com/gjkerns/STAT5840.git
</pre>

</div>
</div>
