{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPDjOw3DV1Lq+tkbaEChfws",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gjkerns/gjkerns.github.io/blob/master/BayesianDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bayesian Statistics Demo\n",
        "\n",
        "This notebook is meant to be a quick demo of some of the main concepts of Bayesian statistics and some of the primary statistical methods that a Bayesian statistician uses.  The discussion follows Chapter 2 of Richard McElreath's book *Statistical Rethinking* 2nd Ed."
      ],
      "metadata": {
        "id": "uz2prGag_7aI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment preliminaries\n",
        "\n",
        "You will need many R packages to do Bayesian statistics.  Below are commands to install a `slim` version of the `rethinking` package that will work for the purposes of this demo but we will need to do some other work to do hardcore Bayesian stuff later.\n",
        "\n",
        "The commands below are for a slim version of the `rethinking` package for your personal laptop or desktop computer.  It looks like the Colab `R` runtime comes with all dependencies out of the box but you will still need to install the `rethinking` package (the 2nd command below) before this notebook will run on Google Colab.  The good news is the installation only takes a couple of minutes to complete.  Note you will need to reinstall the `rethinking` package repeatedly for each runtime."
      ],
      "metadata": {
        "id": "Ei5nw3_HAEPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This line will be required to install on a personal laptop\n",
        "# install.packages(c(\"coda\",\"mvtnorm\",\"devtools\",\"loo\",\"dagitty\"))"
      ],
      "metadata": {
        "id": "JEgb84Fq31J9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# need this one for Google Colab\n",
        "devtools::install_github(\"rmcelreath/rethinking@slim\")"
      ],
      "metadata": {
        "id": "h9s94Y4L31Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One-parameter problem\n",
        "\n",
        "We are going to focus on a one parameter problem and look at several approaches to attack the problem.  In the real world you will have multiple parameters and your models will be more complicated but the techniques will be the same---just more extended.\n",
        "\n",
        "\n",
        "**Parameter of interest:**\n",
        "$$\n",
        "p = \\text{proportion of Earth covered by water}\n",
        "$$\n",
        "\n",
        "Our **random experiment** will be to toss a physical model Earth globe and see whether it hits the ground on an area of water (W) or land (L). The idea is that if Earth is covered by proportion $p$ of water then we would expect the globe to land on water approximately $p$ proportion of the time in the long run. If $\\text{water}=\\text{Success}$, let\n",
        "$$\n",
        "W = \\#\\text{ successes in $n$ trials}\n",
        "$$\n",
        "Then $W$ has what is called a *binomial distribution* which we denote by $W\\sim\\text{binom(size = $n$, prob = $p$)}$ with\n",
        "$$\n",
        "\\Pr(W=w) = \\binom{n}{w} p^{w}(1 - p)^{n-w},\\ w=0,1,2,\\ldots,n.\n",
        "$$\n",
        "\n",
        "The `R` command to evaluate the binomial distribution is:\n",
        "\n",
        "`dbinom(w, size = n, prob = p)`\n",
        "\n"
      ],
      "metadata": {
        "id": "WA_DKD5A9rHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prior distributions and Bayes' Rule\n",
        "\n",
        "We know there is some water on the Earth ($p > 0$) and we know that the polar icecaps haven't melted yet ($p < 1$), so that is, we know that $0 < p < 1$.  A valid prior is any probability distribution on $[0,1]$ or $(0,1)$ that reflects our personal beliefs regarding the value of $p$ before having observed any data.\n",
        "\n",
        "We next perform our random experiment to collect data.  Then we use **Bayes' Rule** to **update** our prior distribution to our **posterior distribution**, that is, our updated beliefs about $p$ after having observed the data.  Bayes' Rule looks like this in shorthand:\n",
        "$$\n",
        "\\text{Posterior} \\propto \\text{Prior} \\times \\text{Likelihood}\n",
        "$$\n",
        "\n",
        "There are four (4) main ways to compute posteriors:\n",
        "\n",
        "0. Mathematics/Calculus (analytical)\n",
        "1. Grid approximation\n",
        "2. Quadratic approximation\n",
        "3. Markov Chain Monte Carlo\n",
        "\n",
        "**Example.**  Suppose we do the globe experiment, and from tossing the globe 9 times we observe the data WLWWWLWLW, that is, six times it landed on `W` and 3 times it landed on `L`.  Therefore $n = 9$ and $w = 6$."
      ],
      "metadata": {
        "id": "JeTllzjU9rLc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Grid approximation\n",
        "\n",
        "The idea is we set up a grid of values for $p$ on the range $[0,1]$, and then specify what we want the prior to be on those grid values.  Higher prior probability on a particular value of $p$ means we have higher prior belief that $p$ is that value.\n",
        "\n",
        "Then we observe data and use Bayes' Rule to compute our posterior distribution on those grid values.  The code for it is pretty easy.\n",
        "\n",
        "- Likelihood of $p$: given by the code $\\text{dbinom(6, size = 9, prob = $p$)}$\n",
        "  - the likelihood is a function of $p$\n",
        "- Prior: Here we choose 20 equally spaced grid points.\n",
        "  - make $p$ equally likely\n",
        "  - experiment with different priors (we will look at 3 different priors)\n",
        "\n",
        "Advantages of grid approximation are that you can use pretty much any prior distribution that you can imagine and the output is relatively easy to understand.  Some drawbacks are that the method doesn't extend easily to multi-parameter problems and the more complicated models can be computationally intensive, sometimes prohibitively so.\n"
      ],
      "metadata": {
        "id": "5kyzSqdq7EL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Uniform prior\n",
        "\n",
        "Let's start with a **uniform prior**.  This takes all values of $p$ to be equally likely.  Since we are using 20 grid points, each possible value of $p$ with have prior probability of $1/20$."
      ],
      "metadata": {
        "id": "e53k_Y8Z-PQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define grid from 0 to 1\n",
        "p_grid <- seq( from=0 , to=1 , length.out=20 )\n",
        "\n",
        "# define prior, all equally likely\n",
        "prior <- rep( 1 , 20 )\n",
        "\n",
        "# compute likelihood at each value in grid\n",
        "likelihood <- dbinom( 6 , size=9 , prob=p_grid )\n",
        "\n",
        "# compute product of likelihood and prior\n",
        "unstd.posterior <- likelihood * prior\n",
        "\n",
        "# standardize the posterior, so it sums to 1\n",
        "posterior <- unstd.posterior / sum(unstd.posterior)"
      ],
      "metadata": {
        "id": "fuOx-4m_31TG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will make a plot of our prior distribution."
      ],
      "metadata": {
        "id": "anviBtst-S7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot( p_grid , prior/sum(prior) , type=\"b\" ,\n",
        "    xlab=\"probability of water\" , ylab=\"prior probability\" ,\n",
        "    main=\"Uniform prior\")\n",
        "mtext( \"20 grid points\" )"
      ],
      "metadata": {
        "id": "4MG0EuFB5-Vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that our prior distribution is *uniformly* flat at the value 0.05.  Next, we will make a plot of our posterior distribution after observing water $w = 6$ times after tossing the globe $n = 9$ times."
      ],
      "metadata": {
        "id": "yx1HiZGT-Xoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot( p_grid , posterior , type=\"b\" ,\n",
        "    xlab=\"probability of water\" , ylab=\"posterior probability\",\n",
        "    main = \"Posterior with w=6 and n=9, Uniform prior\")\n",
        "mtext( \"20 grid points\" )"
      ],
      "metadata": {
        "id": "YlBXv94u5-b8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Truncated uniform prior\n",
        "\n",
        "Let's try a different prior, one that is truncated to put only prior probability on values of $p \\geq 0.5$."
      ],
      "metadata": {
        "id": "eUPZTJJO-cc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define truncated prior\n",
        "prior <- ifelse( p_grid < 0.5 , 0 , 1 )\n",
        "\n",
        "# compute likelihood at each value in grid\n",
        "likelihood <- dbinom( 6 , size=9 , prob=p_grid )\n",
        "\n",
        "# compute product of likelihood and prior\n",
        "unstd.posterior <- likelihood * prior\n",
        "\n",
        "# standardize the posterior, so it sums to 1\n",
        "posterior <- unstd.posterior / sum(unstd.posterior)"
      ],
      "metadata": {
        "id": "QmiSJqad6JNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's plot the truncated prior."
      ],
      "metadata": {
        "id": "EUCZb_cY-p6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot( p_grid , prior/sum(prior) , type=\"b\" ,\n",
        "    xlab=\"probability of water\" , ylab=\"prior probability\",\n",
        "    main=\"Truncated uniform prior\")\n",
        "mtext( \"20 grid points\" )"
      ],
      "metadata": {
        "id": "m6M5tRTr6JTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will plot the posterior, which we notice is also truncated."
      ],
      "metadata": {
        "id": "IJTKefue-uo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot( p_grid , posterior , type=\"b\" ,\n",
        "    xlab=\"probability of water\" , ylab=\"posterior probability\",\n",
        "    main = \"Posterior with w=6 and n=9, from truncated prior\" )\n",
        "mtext( \"20 points\" )"
      ],
      "metadata": {
        "id": "Pz3xrdIJ5-iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Double exponential prior\n",
        "\n",
        "Next let's try a \"double exponential\" prior, which tails off on either side of $p = 0.5$."
      ],
      "metadata": {
        "id": "wZM6Dae35-40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define prior\n",
        "prior <- exp( -5*abs( p_grid - 0.5 ) )\n",
        "\n",
        "# compute likelihood at each value in grid\n",
        "likelihood <- dbinom( 6 , size=9 , prob=p_grid )\n",
        "\n",
        "# compute product of likelihood and prior\n",
        "unstd.posterior <- likelihood * prior\n",
        "\n",
        "# standardize the posterior, so it sums to 1\n",
        "posterior <- unstd.posterior / sum(unstd.posterior)"
      ],
      "metadata": {
        "id": "_XmTj2G86XVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we plot the double exponential prior"
      ],
      "metadata": {
        "id": "PmIOfWUu_BIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot( p_grid , prior/sum(prior) , type=\"b\" ,\n",
        "    xlab=\"probability of water\" , ylab=\"prior probability\",\n",
        "    main=\"Double exponential prior\")\n",
        "mtext( \"20 grid points\" )"
      ],
      "metadata": {
        "id": "b7nBlcJv6Xa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's take a look at the new posterior."
      ],
      "metadata": {
        "id": "r4VwVI-9_H6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot( p_grid , posterior , type=\"b\" ,\n",
        "    xlab=\"probability of water\" , ylab=\"posterior probability\",\n",
        "    main = \"Posterior with w=6 and n=9, double exponential prior\")\n",
        "mtext( \"20 points\" )"
      ],
      "metadata": {
        "id": "gtUsL6176Xg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quadratic approximation\n",
        "\n",
        "Grid approximation is okay, but there are fancier ways to find posteriors.  A better way is to approximate the posterior with a closely fitting bell curve.  The idea: as $n \\to \\infty$, the Posterior will converge $\\to$ normal distribution:\n",
        "$$\n",
        "\\text{Posterior} \\to \\mathrm{e}^{-(p - \\mu)^2/\\sigma^2}\n",
        "$$\n",
        "Then\n",
        "$$\n",
        "\\ln (\\text{Posterior}) \\to -\\frac{(p - \\mu)^2}{2\\sigma^2},\\ (\\text{a parabola})\n",
        "$$\n",
        "Next we use the computer to find the best fitting parabola, then estimate and report $\\mu$ and $\\sigma$.  This procedure is called MAP, for *Maximum a Posteriori*.\n",
        "\n",
        "### How to do it with R"
      ],
      "metadata": {
        "id": "s4YFUGLb_O7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "library(rethinking)\n",
        "globe.qa <- quap(\n",
        "    alist(\n",
        "        W ~ dbinom( W+L, p) ,  # binomial likelihood\n",
        "        p ~ dunif(0,1)         # uniform prior\n",
        "    ) ,\n",
        "    data=list(W=6,L=3) )"
      ],
      "metadata": {
        "id": "8zVC_X9x6dPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We stored the results of our computation in an object named `globe.qa`.  We summarize the object with the `precis()` function."
      ],
      "metadata": {
        "id": "rEtys9pP_VUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# display summary of quadratic approximation\n",
        "precis( globe.qa )"
      ],
      "metadata": {
        "id": "swgJsTgk6dVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above output is saying that we can approximate the posterior in this problem with a normal distribution (bell curve) that has a mean of approximately 0.66 and a standard deviation approximately 0.16.  There is a whole lot more we can do with this output and a whole bunch more stored in the `globe.qa` object that we haven't discussed yet, but for many problems these kinds of computations are good enough and it is a whole lot quicker/easier than any of the other methods we will encounter.\n",
        "\n",
        "Unfortunately, quadratic approximation is a bit limited to well-behaved models, and well-behaved data.  For some of the more complicated things you will probably be working on quadratic approximation is likely not going to be good enough---we'll see."
      ],
      "metadata": {
        "id": "-i2fjIhIYFZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analytical posteriors\n",
        "\n",
        "We can also find posterior distributions with mathematical methods, differential and integral calculus, and the like.  Here is what that looks like for this simple one parameter problem and a uniform prior.  The prior distribution $\\pi(p)$ has the formula\n",
        "$$\n",
        "\\pi(p) = 1,\\ 0 < p < 1. \\quad \\text{PRIOR}\n",
        "$$\n",
        "And the likelihood function $f(w\\vert p)$ takes the mathematical form\n",
        "$$\n",
        "f(w\\vert p) = \\binom{n}{w} p^{w}(1 - p)^{n-w},\\quad \\text{LIKELIHOOD}\n",
        "$$\n",
        "Then our Posterior distribution $\\pi(p\\vert w)$ is computed by means of Bayes' Rule like this:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\pi(p\\vert w)\\propto & \\ 1\\cdot\\binom{n}{w}p^{w}(1-p)^{n-w},\\\\\n",
        "\\propto & \\ p^{(w+1)-1}(1-p)^{(n-w+1)-1},\\\\\n",
        "= & \\ \\text{Beta(df1 = $w+1$, df2 = $n-w+1$)}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "This so-called \"Beta distribution\" is a famous and popular family of models that mathematicians find convenient to get analytical solutions to Bayesian problems.\n",
        "\n",
        "\n",
        "### How to do it with R"
      ],
      "metadata": {
        "id": "wPoHgwP7_hi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# analytical solution plotted out\n",
        "W <- 6\n",
        "L <- 3\n",
        "curve( dbeta( x , W+1 , L+1 ) , from=0 , to=1, main = \"Analytical Beta Posterior when w=6, n=9 with uniform prior\" )\n",
        "# quadratic approximation\n",
        "curve( dnorm( x , 0.67 , 0.16 ) , lty=2 , add=TRUE )\n",
        "mtext( \"dotted line is normal approximation\" )"
      ],
      "metadata": {
        "id": "Fczeh_x56dbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Markov Chain Monte Carlo methods\n",
        "\n",
        "For the hardcore stuff there will not be any analytical solutions available to us and we will need more powerful tools than grid approximation.  Furthermore, in hard problems, the normal approximation doesn't fit very well.  Enter Markov Chain Monte Carlo (MCMC) methods to the rescue.  This topic is covered in Chapter 9 of *Statistical Rethinking*.  The algorithm used below is known as the *Metropolis Algorithm*.\n"
      ],
      "metadata": {
        "id": "7-njtB2Y_nrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Metropolis algorithm example\n",
        "# basic accept-reject process\n",
        "n_samples <- 1000\n",
        "p <- rep( NA , n_samples )\n",
        "p[1] <- 0.5\n",
        "W <- 6\n",
        "L <- 3\n",
        "for ( i in 2:n_samples ) {\n",
        "    p_new <- rnorm( 1 , p[i-1] , 0.1 )\n",
        "    if ( p_new < 0 ) p_new <- abs( p_new )\n",
        "    if ( p_new > 1 ) p_new <- 2 - p_new\n",
        "    q0 <- dbinom( W , W+L , p[i-1] )\n",
        "    q1 <- dbinom( W , W+L , p_new )\n",
        "    p[i] <- ifelse( runif(1) < q1/q0 , p_new , p[i-1] )\n",
        "}\n",
        "\n",
        "## next we plot a density estimate and the theoretical curve on the same graph\n",
        "dens( p , xlim=c(0,1) ,main = \"Posterior with w=6, n=9, uniform prior\")\n",
        "curve( dbeta( x , W+1 , L+1 ) , lty=2 , add=TRUE )\n",
        "mtext( \"dotted line is true beta posterior\" )"
      ],
      "metadata": {
        "id": "zMgI_3f-6oR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look how the wiggly density curve approximates the actual, true Beta posterior shown by the dotted line.  The approximation isn't very good because the MCMC sampler only used `n_samples = 1000`.\n",
        "\n",
        "You can rerun the code above yourself for a bunch of different values of `n_samples` to see how the density approximation gets better and better for larger values of `n_samples`.\n",
        "\n"
      ],
      "metadata": {
        "id": "bAokLKy2_1zo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HMC methods and beyond...\n",
        "\n",
        "The MCMC methods described above have been around for some decades and in recent years there has been some significant progress and major speed-ups to the methods, but this means even more parameters and requirements for even higher performance computing.  The latest trend in Bayesian statistics deals with Hamiltonian Monte Carlo (HMC) methods and a programming interface called `Stan`.  See here for more:\n",
        "\n",
        "https://mc-stan.org/\n",
        "\n",
        "Stan interfaces with the most popular data analysis languages (R, Python, shell, MATLAB, Julia, Stata) and runs on all major platforms (Linux, Mac, and Windows).  We very well may need Stan before it's all said and done."
      ],
      "metadata": {
        "id": "GA5PJr32EOu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bibliography\n",
        "\n",
        "McElreath, R. (2018). *Statistical rethinking: A Bayesian course with examples in R and Stan.* Chapman and Hall/CRC."
      ],
      "metadata": {
        "id": "PgPp6TMDEPJJ"
      }
    }
  ]
}