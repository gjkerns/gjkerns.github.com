[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics, plain and sample",
    "section": "",
    "text": "The Queen of Hearts\n\n\n\n\n\n\n\nprobability\n\n\nclassical\n\n\n\n\nQuick look at an intriguing game.\n\n\n\n\n\n\nMar 6, 2024\n\n\nG. Jay Kerns\n\n\n\n\n\n\n  \n\n\n\n\nRip Van Winkle\n\n\n\n\n\n\n\nnews\n\n\n\n\nWow. A lot has happened in the past 10 years.\n\n\n\n\n\n\nDec 7, 2022\n\n\nG. Jay Kerns\n\n\n\n\n\n\n  \n\n\n\n\nPower and Sample Size for Repeated Measures ANOVA with R\n\n\n\n\n\n\n\nrstats\n\n\nclassical\n\n\n\n\nA colleague is working on a paper and we recently got together to work through some of the statistical issues.\n\n\n\n\n\n\nJan 20, 2012\n\n\nG. Jay Kerns\n\n\n\n\n\n\n  \n\n\n\n\nEstimating a normal mean with a cauchy prior\n\n\n\n\n\n\n\nrstats\n\n\nbayesian\n\n\nmc\n\n\n\n\nThis is an example (with a Bayesian spin) illustrating one way to compute a complicated expectation (that is, an integral) by the Monte Carlo method.\n\n\n\n\n\n\nAug 27, 2011\n\n\nG. Jay Kerns\n\n\n\n\n\n\n  \n\n\n\n\nMaiden Voyage\n\n\n\n\n\n\n\nnews\n\n\nrstats\n\n\n\n\nThis is the first post. A mission statement, of sorts.\n\n\n\n\n\n\nAug 23, 2011\n\n\nG. Jay Kerns\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a blog written by me. It isn’t meant as a political platform, personal diary, or much of anything aside from my prattling on about topics that interest me in statistics, data science, and data analytics."
  },
  {
    "objectID": "posts/2011-08-23-maiden/index.html",
    "href": "posts/2011-08-23-maiden/index.html",
    "title": "Maiden Voyage",
    "section": "",
    "text": "Me. I’m an associate professor of Statistics at Youngstown State University in Youngstown, Ohio, USA. I’ve been using R for about 7 years, Emacs about 3 years, git about 1 year, and Org-Mode for less than a year."
  },
  {
    "objectID": "posts/2011-08-27-estimate-normal-mean/index.html",
    "href": "posts/2011-08-27-estimate-normal-mean/index.html",
    "title": "Estimating a normal mean with a cauchy prior",
    "section": "",
    "text": "When doing statistics the Bayesian way, we are sometimes bombarded with complicated integrals that do not lend themselves to closed-form solutions. This used to be a problem. Nowadays, not so much. This post illustrates how a person can use the Monte Carlo method (and R) to get a good estimate for an integral that might otherwise look unwieldy at first glance. Of course, in this example, the integral isn’t very complicated. But the /method/ works the same, regardless of the mess in which we find ourselves. The current example is derived from one in Monte Carlo Statistical Methods by Robert/Casella (in Chapter 4). For that matter, check out their Introducing Monte Carlo Methods with R.\nSuppose we have one observation \\(X \\sim N(\\theta,1)\\) but we have a (robust) prior distribution on \\(\\theta\\), namely, \\(\\theta \\sim \\mathrm{Cauchy}(0,1)\\). We would like to update our beliefs about \\(\\theta\\) based on the information provided by \\(x\\). So our likelihood is \\[\nf(x\\vert\\theta) = \\frac{1}{\\sqrt{2\\pi}}\\exp \\left[-\\frac{1}{2}(x - \\theta)^2  \\right],\n\\] and our prior is \\[\ng(\\theta) = \\frac{1}{\\pi}\\frac{1}{(1 + \\theta^{2})}.\n\\] The posterior distribution is proportional to the likelihood times prior, that is, \\[\ng(\\theta\\vert x) \\propto \\exp \\left[-\\frac{1}{2}(x - \\theta)^2  \\right] \\frac{1}{(1 + \\theta^{2})},\n\\] with the proportionality constant being the reciprocal of \\[\nC = \\int \\exp \\left[-\\frac{1}{2}(x - \\theta)^2  \\right] \\frac{1}{(1 + \\theta^{2})} \\mathrm{d} \\theta.\n\\] Our point estimate (or best guess) for \\(\\theta\\) will be just the posterior mean, given by \\[\n\\mathbb{E} (\\theta \\vert \\mbox{data}) = \\frac{ \\int \\theta \\exp \\left[-\\frac{1}{2}(x - \\theta)^2  \\right] \\frac{1}{(1 + \\theta^{2})} \\mathrm{d} \\theta }{C}.\n\\]\nWe notice that the integrand for \\(C\\) looks like something times a Cauchy PDF, where the something (let’s call it \\(h\\)) is \\[\nh(\\theta) = \\pi \\exp \\left[-\\frac{1}{2}(x - \\theta)^2  \\right],\n\\] so one way to use the Monte Carlo method follows.\n\n\nGiven observed data \\(X=x\\),\n\nSimulate a bunch of Cauchys, \\(\\theta_{1},\\theta_{2},\\ldots,\\theta_{m}\\), say.\nEstimate the integral in the denominator with \\[\\frac{1}{m}\\sum_{i=1}^{m} \\pi\\exp\\left[-\\frac{1}{2}(x - \\theta_{i})^2 \\right].\\]\nEstimate the integral in the numerator with \\[\\frac{1}{m}\\sum_{i=1}^{m} \\pi\\,\\theta_{i} \\exp \\left[-\\frac{1}{2}(x - \\theta_{i})^2  \\right].\\]\nTake the ratio, and we’re done.\n\nThe Strong Law of Large Numbers says that the averages in 2 and 3 both converge to where they should, so the ratio should converge to the right place as well."
  },
  {
    "objectID": "posts/2011-08-27-estimate-normal-mean/index.html#how-to-do-it-with-r",
    "href": "posts/2011-08-27-estimate-normal-mean/index.html#how-to-do-it-with-r",
    "title": "Estimating a normal mean with a cauchy prior",
    "section": "How to do it with R",
    "text": "How to do it with R\nThe following is an R script which does the above. For laughs, let’s suppose that we observed \\(X=3\\).\n\nset.seed(1)       # makes the experiment reproducible\nm <- 2000         # number of simulated values\nx <- 3            # observed data\n\n# Now simulate some random variables\n\ntheta <- rcauchy(m)                   # simulate m standard Cauchys \nh <- pi * exp(-0.5*(x - theta)^2)     # who wants to write this over and over\n\nConstant <- mean(h)                   # estimate normalizing constant \npost.mean <- mean(theta * h)/mean(h)  # estimate posterior mean #+end_src"
  },
  {
    "objectID": "posts/2011-08-27-estimate-normal-mean/index.html#at-the-command-prompt",
    "href": "posts/2011-08-27-estimate-normal-mean/index.html#at-the-command-prompt",
    "title": "Estimating a normal mean with a cauchy prior",
    "section": "At the command prompt",
    "text": "At the command prompt\nAfter copy-pasting the above into an R session we can see what the results were with an additional\n\nConstant\n\n[1] 0.3724711\n\npost.mean\n\n[1] 2.334232\n\n\nFor this simple example we can actually calculate what the true values are (to machine precision) with the following: for the constant \\(C\\) we get\n\nf <- function(x) exp(-0.5*(x - 3)^2)/(1 + x^2)\nintegrate(f, -Inf, Inf)\n\n0.3416549 with absolute error < 1.3e-07\n\n\nso our estimate of \\(C\\) overshot the mark by about 0.03, and in the posterior mean case we get\n\ng <- function(x) x * f(x)\nintegrate(g, -Inf, Inf)$value / integrate(f, -Inf, Inf)$value\n\n[1] 2.285139\n\n\nso our estimate of the posterior mean was around 0.05 too high. If we would like to get better estimates, we could increase the value of m = 2000 to something higher (assuming these things are actually converging someplace)."
  },
  {
    "objectID": "posts/2011-08-27-estimate-normal-mean/index.html#are-we-waiting-long-enough",
    "href": "posts/2011-08-27-estimate-normal-mean/index.html#are-we-waiting-long-enough",
    "title": "Estimating a normal mean with a cauchy prior",
    "section": "Are we waiting long enough?",
    "text": "Are we waiting long enough?\nOur estimates were a little bit off; we might like to take a look at a plot to see how we’re doing – is this thing really converging like we’d expect? We can look at a running average plot to assess convergence. If the plot bounces around indeterminably, that’s bad, but if it settles down to a finite constant, that’s better. Here’s a quick way to check this out with ggplot graphics.\n\nrc <- cumsum(h)/seq_along(h)          # running mean of C \nrpm <- cumsum(h * theta)/cumsum(h)    # running posterior mean\n\nNow we plot the results.\n\nlibrary(ggplot2)\nlibrary(grid)\nA <- data.frame(iter = 1:m, rc = rc, rpm=rpm)\nlibrary(reshape)\nA.short <- melt(A[3:200, ], id=\"iter\")\na <- ggplot(A.short, aes(iter, value, colour=variable)) + geom_line() +\n      labs(title = \"First 200\")\nA.long <- melt(A, id=\"iter\")\nb <- ggplot(A.long, aes(iter, value, colour=variable)) + geom_line() +\n      labs(title = \"All 2000 iterations\")\ngrid.newpage()\npushViewport(viewport(layout = grid.layout(1, 2, widths = unit(c(3,5),\"null\"))))\nvplayout <- function(x, y)\nviewport(layout.pos.row = x, layout.pos.col = y)\nprint(a, vp = vplayout(1, 1))\nprint(b, vp = vplayout(1, 2))\n\n\n\n\nFigure 1: Running averages for assessing convergence of the estimators.\n\n\n\n\nIn this example, the estimates look to be still unstable at around m = 200, but by the time we reach m = 2000 they look to have pretty much settled down. Here we knew what the true values were, so we could tell immediately how well we were doing. On the battlefield we will not be so lucky. In general, with Monte Carlo estimates like these it is wise to take a look at some plots to judge the behavior of our estimators. If our plot looks more like the one on the left, then we should consider increasing the sample size. If our plot looks more like the one on the right, then maybe we would be satisfied with “close enough”. (We can always wait longer, tight purse-strings notwithstanding.)"
  },
  {
    "objectID": "posts/2011-08-27-estimate-normal-mean/index.html#other-approaches",
    "href": "posts/2011-08-27-estimate-normal-mean/index.html#other-approaches",
    "title": "Estimating a normal mean with a cauchy prior",
    "section": "Other approaches",
    "text": "Other approaches\nWhen we were looking to estimate \\(C\\) we noticed that the integrand was something times a Cauchy distribution. If we look again, we can see that the same integrand also looks like a normal distribution times something. So, another approach would be to simulate a bunch of normals and average the new somethings. Do we get the same answer (in the limit)?\nYes, of course. It turns out, the approach simulating normals does a little bit better than the one simulating Cauchys, but they’re really pretty close. Check out chapter 4 of Monte Carlo Statistical Methods for discussion on this."
  },
  {
    "objectID": "posts/2011-08-27-estimate-normal-mean/index.html#where-to-find-more",
    "href": "posts/2011-08-27-estimate-normal-mean/index.html#where-to-find-more",
    "title": "Estimating a normal mean with a cauchy prior",
    "section": "Where to find more…",
    "text": "Where to find more…\nThe above is a variant of an example we did in STAT 5840, Statistical Computing. The entire course is available online at GitHub. Go to the Downloads for a .zip file or .tar.gz. Or, if you have git installed, you can get (git? har har) it all with\ngit clone git://github.com/gjkerns/STAT5840.git"
  },
  {
    "objectID": "posts/2012-01-20-power-sample-size/index.html",
    "href": "posts/2012-01-20-power-sample-size/index.html",
    "title": "Power and Sample Size for Repeated Measures ANOVA with R",
    "section": "",
    "text": "One of my colleagues is an academic physical therapist (PT), and he’s working on a paper to his colleagues related to power, sample size, and navigating the thicket of trouble that surrounds those two things. We recently got together to walk through some of the issues, and I thought I would share some of the wildlife we observed along the way. If you just want the code and don’t care about the harangue, see this gist on GitHub."
  },
  {
    "objectID": "posts/2012-01-20-power-sample-size/index.html#the-problem",
    "href": "posts/2012-01-20-power-sample-size/index.html#the-problem",
    "title": "Power and Sample Size for Repeated Measures ANOVA with R",
    "section": "The problem",
    "text": "The problem\nSuppose you are a PT, and you’ve come up with a brand new exercise method that you think will decrease neck pain, say. How can you demonstrate that your method is effective? Of course, you collect data and show that people using your method have significantly lower neck pain than those from a control group.\nThe standard approach in the PT literature to analyze said data is repeated measures ANOVA. (Yes, those guys should really be using mixed-effects models, but those haven’t quite taken off yet.) There are two groups: the “Treatment” group does your new exercise method, and a “Sham” group does nothing (or just the placebo exercise method). For each Subject, you measure their pain at time 0, 15 minutes, 48 hours, and 96 hours. Pain is measured by an index (there are several); the one we’re using is something called NDI, which stands for “Neck Disability Index”. The index ranges from 0 to 100 (more on this later). There is some brief information about the index here.\nNow comes the question: how many people should you recruit for your study? The answer is: it depends. “On what?” Well, it depends on how good the statistical test is, and how good your method is, but more to the point, it depends on the effect size, that is, how far apart the two groups are, given that the method actually works.\nI encounter some variant of this question a lot. I used to go look for research papers where somebody’s worked out the F-test and sample sizes required, and pore over tables and tables. Then I resorted to online calculators (the proprietary versions were too expensive for my department!), which are fine, but they all use different notation and it takes a lot of time poring through documentation (which is often poor, pardon the pun) to recall how it works. And I was never really sure whether I’d got it right, or if I had screwed up with a parameter somewhere.\nSome of the calculators advertise Cohen’s effect sizes, which are usually stated something like “small”, “medium”, and “large”, with accompanying numerical values. Russell Lenth calls these “T-shirt effect sizes”. I agree with him.\nNowadays the fashionable people say, “Just run a simulation and estimate the power,” but the available materials online are scantly detailed. So my buddy and I worked it all out from start to finish for this simple example, in the hopes that by sharing this information people can get a better idea of how to do it the right way, the first time."
  },
  {
    "objectID": "posts/2012-01-20-power-sample-size/index.html#how-to-attack-it",
    "href": "posts/2012-01-20-power-sample-size/index.html#how-to-attack-it",
    "title": "Power and Sample Size for Repeated Measures ANOVA with R",
    "section": "How to attack it",
    "text": "How to attack it\nThe avenue of attack is simple: for a given sample size,\n\nuse prior research and practitioner experience to decide what difference would be “meaningful” to detect,\nsimulate data consistent with the above difference and run the desired statistical test to see whether or not it rejected, and\nrepeat step 2 hundreds of times. An estimate of the power (for that sample size) is the proportion of times that the test rejected.\n\nIf the power isn’t high enough, then increase the given sample size and start over. The value we get is just an estimate of the power, but we can increase the precision of our estimate by increasing the number of repetitions in step 3.\nWhat you find when you start down this path is that there is a lot of information required to be able to answer the question. Of course, this information had been hiding behind the scenes all along, even with those old research papers and online calculators, but the other methods make it easy to gloss over the details, or they’re so complicated that researchers will give up and fall back to something like Cohen’s T-shirt effect sizes."
  },
  {
    "objectID": "posts/2012-01-20-power-sample-size/index.html#now-for-the-legwork",
    "href": "posts/2012-01-20-power-sample-size/index.html#now-for-the-legwork",
    "title": "Power and Sample Size for Repeated Measures ANOVA with R",
    "section": "Now for the legwork",
    "text": "Now for the legwork\nThe details we need include: A) prior knowledge of how average pain decreases for people in the Sham group, B) some idea about the variability of scores, C) how scores would be correlated with one another over time, and D) how much better the Treat group would need to be in order for the new procedure to be considered clinically meaningful.\nAs a first step, the PT sat down and filled in the following table.\n\n\n\n\n0 hrs\n15 min\n48 hrs\n96 hrs\n\n\n\n\nTreat\n37\n32\n20\n15\n\n\nSham\n37\n32\n25\n22\n\n\n\nAll of the entries in the above table represent population mean NDI scores for people in the respective groups at the respective measurement times, and were filled in based on prior research and educated guesses by the PT. It was known from other studies that NDI scores have a standard deviation of around 12, and those have been observed to decrease over time.\nNote: we could have assumed a simpler model for the means. For example, we could have assumed that mean NDI was linear, with possibly different slopes/intercepts for the Treat/Sham groups. Prior info available to the PT said that such an assumption wasn’t reasonable for this example.\nRepeated measures designs assume sphericity for the exact F tests to hold, so we need to specify a variance for the differences, \\(\\mathrm{Var}(X_{i} - X_{j})\\), and sphericity says this variance should be the same for all time points \\(i\\) and \\(j\\). As it turns out, this last choice implicitly determines all of the remaining covariance structure. We set this standard deviation to \\(9\\)."
  },
  {
    "objectID": "posts/2012-01-20-power-sample-size/index.html#finally-we-do-some-coding",
    "href": "posts/2012-01-20-power-sample-size/index.html#finally-we-do-some-coding",
    "title": "Power and Sample Size for Repeated Measures ANOVA with R",
    "section": "Finally we do some coding",
    "text": "Finally we do some coding\nWe are now ready to turn on the computer. We first intialize the parameters we’ll need, next we set up the independent variable data, then we do the simulation, and finally we rinse-and-repeat. Let’s go.\n\nset.seed(1)\nnPerGroup <- 10\nnTime     <- 4\nmuTreat   <- c(37, 32, 20, 15)\nmuSham    <- c(37, 32, 25, 22)\nstdevs    <- c(12, 10, 8, 6)\nstdiff    <- 9\nnSim      <- 500\n\nAll of the above should be self-explanatory. Next comes setting up the data - creatively named theData - for the independent variables. Just for the sake of argument I used code to generate the data frame, but we wouldn’t have had to. We could have imported an external text file had we wished.\n\nSubject <- factor(1:(nPerGroup*2))\nTime <- factor(1:nTime, labels = c(\"0min\", \"15min\", \"48hrs\", \"96hrs\"))\n\ntheData <- expand.grid(Time, Subject)\nnames(theData) <- c(\"Time\", \"Subject\")\n\ntmp <- rep(c(\"Treat\", \"Sham\"), each = nPerGroup * nTime)\ntheData$Method <- factor(tmp)\n\nAgain, the above should be self-explanatory for the most part. The data are in “long” form, where each subject appears over multiple rows. In fact, let’s take a look at the data frame to make sure it looks right.\n\nhead(theData)\n\n   Time Subject Method\n1  0min       1  Treat\n2 15min       1  Treat\n3 48hrs       1  Treat\n4 96hrs       1  Treat\n5  0min       2  Treat\n6 15min       2  Treat\n\n\nLookin’ good. Now for the fun part. We generate the single remaining column, the NDI scores. The repeated measures model is multivariate normal. The population covariance matrix is a little bit tricky, but it’s not too bad and to make things easy we’ll assume both groups have the same covariance. See the original paper by Huynh and Feldt for details.\n\n# to set up variance-covariance matrix\nones <- rep(1, nTime)\nA <- stdevs^2 %o% ones\nB <- (A + t(A) + (stdiff^2)*(diag(nTime) - ones %o% ones))/2\n\nWe simulate with the mvrnorm function from the MASS package.\n\nlibrary(MASS)\ntmp1 <- mvrnorm(nPerGroup, mu = muTreat, Sigma = B)\ntmp2 <- mvrnorm(nPerGroup, mu = muSham, Sigma = B)\ntheData$NDI <- c(as.vector(t(tmp1)), as.vector(t(tmp2)))\n\nNow that we have our data, we can run the test:\n\naovComp <- aov(NDI ~ Time*Method + Error(Subject/Time), theData)\nsummary(aovComp)\n\n\nError: Subject\n          Df Sum Sq Mean Sq F value Pr(>F)\nMethod     1  207.2   207.2   1.896  0.185\nResiduals 18 1967.5   109.3               \n\nError: Subject:Time\n            Df Sum Sq Mean Sq F value   Pr(>F)    \nTime         3   4871  1623.5  42.709 2.79e-14 ***\nTime:Method  3    251    83.7   2.201   0.0985 .  \nResiduals   54   2053    38.0                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTerrific! For these data, we observe a highly significant Time effect (this should be obvious given our table above), an insignificant Method fixed effect, and an insignificant Time:Method interaction. If we think about our model and what we’re interested in, it’s the interaction which we care about and that which we’d like to detect. If our significance level had been \\(\\alpha = 0.05\\), we would not have rejected this time, but who knows what would happen next time.\nNow it’s time to rinse-and-repeat, which we accomplish with the replicate function. Before we get there, though, let’s look at a plot. There are several relevant ones, but in the interest of brevity let’s satisfy ourselves with an interaction.plot:\n\nwith(theData, interaction.plot(Time, Method, NDI))\n\n\n\n\nEverything is going according to plan. There is definitely a Time effect (the lines both slope downward) but there isn’t any evidence of an interaction (the lines have similar slopes).\nOn to rinse-and-repeat, we first set up the function that runs the test once:\n\nrunTest <- function(){\n  tmp1 <- mvrnorm(nPerGroup, mu = muTreat, Sigma = B)\n  tmp2 <- mvrnorm(nPerGroup, mu = muSham, Sigma = B)\n  theData$NDI <- c(as.vector(t(tmp1)), as.vector(t(tmp2)))\n  aovComp <- aov(NDI ~ Time*Method + Error(Subject/Time), theData)  \n  b <- summary(aovComp)$'Error: Subject:Time'[[1]][2,5]\n  b < 0.05\n}\n\nand finally do the repeating:\n\nmean(replicate(nSim, runTest()))\n\n[1] 0.346\n\n\nWhoa! The power is 0.346? That’s pretty low. We recall that this is just an estimate of power - how precise is the estimate? The standard error of \\(\\hat{p}\\) is approximately \\(\\sqrt{\\hat{p}(1-\\hat{p})/n}\\), so in our case, our estimate’s standard error is approximately 0.021. That means we are approximately 95% confident that the true power at this particular alternative is covered by the interval \\([0.303,0.389]\\).\nStandard practice is to shoot for a power of around \\(\\beta = 0.80\\), so our power isn’t even close to what we’d need. We can increase power by increasing sample size (the parameter nPerGroup). A larger sample size means a longer time needed to run the simulation. Below are some results of running the above script at assorted sample sizes.\n\n\n\nnPerGroup\nPower (estimate)\nSE (approx)\n\n\n\n\n10\n0.346\n0.021\n\n\n20\n0.686\n0.021\n\n\n30\n0.886\n0.014\n\n\n\nNow we’re talking. It looks like somewhere between 20 and 30 subjects per group would be enough to detect the clinically meaningful difference proposed above with a power of 0.80.\nUnfortunately, the joke is on us. Because, as it happens, it’s no small order for a lone, practicing PT (around here) to snare 60 humans with neck pain for a research study. A person would need to be in (or travel to) a heavily populated area, and even then there would be dropout, people not showing up for subsequent appointments.\nSo what can we do?\n\nModify the research details. If we take a closer look at the table, there isn’t an expected difference in the means until 48 hours, so why not measure differently, say, at 0, 48, 96, and 144 hours? Is there something else about the measurement process we could change to decrease the variance?\nUse a different test. We are going with boilerplate repeated-measures ANOVA here. Is that really the best choice? What would happen if we tried the mixed-effects approach?\nTake a second look at the model. We should not only double-check our parameter choices, but rethink: is the repeated-measures model (multivariate normal) the most appropriate? Is it reasonable for the variance of differences at all time pairs to be identical? What about the covariance structure? There are others we could try, such as an autoregressive model (another arrow in the mixed-effects models’ quiver)."
  },
  {
    "objectID": "posts/2012-01-20-power-sample-size/index.html#other-things-to-keep-in-mind",
    "href": "posts/2012-01-20-power-sample-size/index.html#other-things-to-keep-in-mind",
    "title": "Power and Sample Size for Repeated Measures ANOVA with R",
    "section": "Other things to keep in mind",
    "text": "Other things to keep in mind\n\nThis example is simple enough to have done analytically; we didn’t have to simulate anything at all.\nEven if the example hadn’t been simple, we could still have searched for an approximate analytic solution which, if nothing else, might have given some insight into the power function’s behavior.\nWe could have adjusted all the means upward by 7 and nothing would have changed. We based our initial values on literature review and clinical expertise.\nWe didn’t bother with contrasts, functional means, or anything else. We just generated data consistent with our null and salient alternative and went on with our business.\nWe could have used whatever test we liked yet the method of attack would have been the same. Multiple comparisons, nested tests, nonparametric tests, whatever. As long as we include the full procedure in runTest, we will get valid estimates of power for that procedure at that alternative.\nWe need to be careful that the test we use (whatever it is) has its significance level controlled. This is easy to check in our example. We can set the means equal (muTreat = muSham) and run the simulation. We should get a power equal to 0.05 (within margin of error). Go ahead, check yourself. In fact, since we only care about the interaction, we could vertically offset the means by any fixed number, not necessarily zero.\nHad we not been careful with our stdevs, our simulated NDIs would have gone negative, particularly at the latter time points. That would not have been reasonable since NDI is nonnegative.\nSimulation is not a silver bullet.\nEffective simulation requires substantial investment of thought into both the probability model and the parameter settings.\nOur model had 13 parameters, and we had 4 more we didn’t even touch[fn:1]. A person could be forgiven for wondering how in the world all of those parameters can be expressively spun into a T-shirt effect size. (They can’t.)\nThe complexity can get out of control quickly. Simulation run times can take forever. The more complicated the model/test the worse it gets.\nInformative simulation demands literature review and content expertise as a prerequisite. Some researchers are unable (due to lack of existing/quality studies) or unwilling (for all sorts of reasons, not all of which are good) to help the statistician fill in the details. For the statistician, this is a problem. If you don’t know anything, then you can’t say anything.\nWe can address uncertainty in our parameter guesses with prior distributions on the parameters. This adds a layer of complexity to the simulation since we must first simulate the parameters before simulating the data. Sometimes there’s no other choice.\nTheory tells us that the standard research designs (including our current one) can usually be re-parameterized by a single non-centrality parameter which ultimately determines the power at any particular alternative. Following our nose, it suggests that our problem is simpler than we’re making it, that if we would just write down the non-centrality parameter (and the right numerator/denominator degrees of freedom), we’d be all set. Yep, we would. Good luck with all… that.\n\n[fn:1] John von Neumann once said, “With four parameters I can fit an elephant, and with five I can make him wiggle his trunk.”"
  },
  {
    "objectID": "posts/2012-01-20-power-sample-size/index.html#references",
    "href": "posts/2012-01-20-power-sample-size/index.html#references",
    "title": "Power and Sample Size for Repeated Measures ANOVA with R",
    "section": "References",
    "text": "References\n\nSee this question on CrossValidated which came up while I was working on this document (I might not have answered so quickly otherwise). Thanks to all who contributed to that discussion.\nConditions Under Which Mean Square Ratios in Repeated Measurements Designs Have Exact F-Distributions. Huynh Huynh and Leonard S. Feldt, Journal of the American Statistical Association, Vol. 65, No. 332 (Dec., 1970), pp. 1582-1589, stable link.\nI found this website while preparing for the initial meeting and got some inspiration from the discussion near the middle.\nThere are several papers on Russell Lenth’s webpage which are good reading.\nI also like this paper. Keselman, H. J., Algina, J. and Kowalchuk, R. K. (2001), The analysis of repeated measures designs: A review. British Journal of Mathematical and Statistical Psychology, 54: 1–20. doi: 10.1348/000711001159357\nMany of the concepts above are explained more formally in my Statistical Computing course which you can get on GitHub.\nTo learn more about Monte Carlo methods with R I recommend Introducing Monte Carlo Methods with R by Robert and Casella. I also like Statistical Computing with R by Rizzo which has a section about simulating power of statistical tests.\nFor the record, here is my sessionInfo.\n\n\nsessionInfo()\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19044)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n[1] MASS_7.3-58.1\n\nloaded via a namespace (and not attached):\n [1] digest_0.6.30   lifecycle_1.0.3 jsonlite_1.8.3  magrittr_2.0.3 \n [5] evaluate_0.18   rlang_1.0.6     stringi_1.7.8   cli_3.4.1      \n [9] renv_0.16.0     vctrs_0.5.1     rmarkdown_2.18  tools_4.2.2    \n[13] stringr_1.5.0   glue_1.6.2      xfun_0.35       yaml_2.3.6     \n[17] fastmap_1.1.0   compiler_4.2.2  htmltools_0.5.3 knitr_1.41"
  },
  {
    "objectID": "posts/2011-08-23-maiden/index.html#what",
    "href": "posts/2011-08-23-maiden/index.html#what",
    "title": "Maiden Voyage",
    "section": "What",
    "text": "What\nI want this blog to be about statistics, plain and sample. No frills, no tomfoolery, just bare-boned statistics from beginning to end. Plus Emacs, ESS, Org-Mode, and R, but that goes without saying."
  },
  {
    "objectID": "posts/2011-08-23-maiden/index.html#when",
    "href": "posts/2011-08-23-maiden/index.html#when",
    "title": "Maiden Voyage",
    "section": "When",
    "text": "When\nI’ve wanted to do this for a long time, but had as of yet convinced myself that I didn’t have time for it. A sabbatical coupled with the renewed energy of a use R! conference can change things considerably."
  },
  {
    "objectID": "posts/2011-08-23-maiden/index.html#how-emacs-org-mode-jekyll-github-blog-r",
    "href": "posts/2011-08-23-maiden/index.html#how-emacs-org-mode-jekyll-github-blog-r",
    "title": "Maiden Voyage",
    "section": "How: Emacs + Org mode + Jekyll + Github = Blog + R!",
    "text": "How: Emacs + Org mode + Jekyll + Github = Blog + R!\nAfter much fiddling and googling I have managed to figure out how to run a blog entirely through Emacs and Git. If you’d like to do the same I recommend reading here and here, with liberal doses of this and that. Ultimately, if you’d like to know how I do it then you can find the org-mode source code for this blog here and you can download the final result here (which still is source code but is as close to final as possible).\nThe bottom line: with this setup I can effortlessly do R code like this:\n\nrnorm(10)\n\n [1] -1.26005550 -0.24153330 -0.26270810  0.11929914 -0.05094026 -0.54149791\n [7]  0.51524809 -0.90041545  0.52012774 -0.23936052\n\n\nAnd can include plots like this:\n\nx = rnorm(100) \ny = rnorm(100)\nplot(y ~ x)\n\n\n\n\nFigure 1: A plot to get things started.\n\n\n\n\nall housed inside a simple, dynamic text file that I can edit with Emacs and can version-control with git. On top of all this, I get LaTeX formatting in HTML via MathJax. Life is good."
  },
  {
    "objectID": "posts/2022-12-07/index.html",
    "href": "posts/2022-12-07/index.html",
    "title": "Rip Van Winkle",
    "section": "",
    "text": "Quidor (1829)\nI originally wanted to title this post “The Old Woman Voyage,” as sort of counterpoint to the original “Maiden Voyage” in 2011, but somehow Rip Van Winkle seems more appropriate. Do you ever feel like you woke up one day to realize the world is completely different from what you thought it was? And the world is so different that it feels like you must have been sleeping so long that the world has passed you by? Yeah? No? Doesn’t matter—that’s how I feel right now."
  },
  {
    "objectID": "posts/2022-12-07/index.html#new-world",
    "href": "posts/2022-12-07/index.html#new-world",
    "title": "Rip Van Winkle",
    "section": "New world",
    "text": "New world\nMaybe it was the pandemic, maybe it was my kids getting older, maybe it was the obliterated landscape of that which we call academia, maybe it was all the turmoil going on all around us, somehow, there is a spooky energy in the air. I blinked my eyes and now it is so much easier to do stuff, like write blogs, code on Github, generate NAMESPACEs and documentation for R packages.\nAnd all of that social media I have shunned for literally a decade is bustling with activity populated by people with dynamite ideas about anything and everything, not all of which are cat pictures and what JLo happens to be eating for breakfast."
  },
  {
    "objectID": "posts/2022-12-07/index.html#new-mission",
    "href": "posts/2022-12-07/index.html#new-mission",
    "title": "Rip Van Winkle",
    "section": "New mission",
    "text": "New mission\nThis blog is supposed to be about statistics, plain and sample. No JLo. No cat pictures. No snarky comments about something a politician said 10 minutes ago, or lamentations about all the turmoil going on in the world. There are too many other people talking about those things everywhere else.\nMaybe this isn’t a new mission, but it is a renewed energy to do what I like and do best: statistics, plain and sample."
  },
  {
    "objectID": "posts/2022-12-07-rip-van-winkle/index.html",
    "href": "posts/2022-12-07-rip-van-winkle/index.html",
    "title": "Rip Van Winkle",
    "section": "",
    "text": "Quidor (1829)\nI originally wanted to title this post “The Old Woman Voyage,” as sort of counterpoint to the original “Maiden Voyage” in 2011, but somehow Rip Van Winkle seems more appropriate. Do you ever feel like you woke up one day to realize the world is completely different from what you thought it was? And the world is so different that it feels like you must have been sleeping so long that the world has passed you by? Yeah? No? Doesn’t matter—that’s how I feel right now."
  },
  {
    "objectID": "posts/2022-12-07-rip-van-winkle/index.html#new-world",
    "href": "posts/2022-12-07-rip-van-winkle/index.html#new-world",
    "title": "Rip Van Winkle",
    "section": "New world",
    "text": "New world\nMaybe it was the pandemic, maybe it was my kids getting older, maybe it was the post-apocalyptic landscape of that which we call academia, maybe it was all the turmoil going on all around us in the world, somehow, there is a spooky energy in the air. I blinked my eyes and now it is so much easier to do stuff, like write blogs, code on Github, generate NAMESPACEs and documentation for R packages.\nAnd all of that social media I have shunned for literally a decade is bustling with activity populated by people with dynamite ideas about anything and everything, not all of which are cat pictures and what JLo happens to be eating for breakfast."
  },
  {
    "objectID": "posts/2022-12-07-rip-van-winkle/index.html#new-mission",
    "href": "posts/2022-12-07-rip-van-winkle/index.html#new-mission",
    "title": "Rip Van Winkle",
    "section": "New mission",
    "text": "New mission\nThis blog is supposed to be about statistics, plain and sample. No JLo. No cat pictures. No snarky comments about something a politician said 10 minutes ago, or lamentations about all the turmoil going on in the world. There are too many other people talking about those things everywhere else.\nMaybe this isn’t a new mission, but it is a renewed energy to do what I like and do best: statistics, plain and sample."
  },
  {
    "objectID": "posts/2024-03-06-queen-of-hearts/index.html",
    "href": "posts/2024-03-06-queen-of-hearts/index.html",
    "title": "The Queen of Hearts",
    "section": "",
    "text": "The Queen\n\n\n\nRules of the game\nLocal customs vary, but here is a general description: the 52 cards (plus 2 Jokers) of a standard deck of playing cards are well-shuffled and posted face down on a high-visibility surface (such as a wall in the local tavern).\nPlayers buy tickets, as many as they want, for $1 each and fill in their names, phone numbers and the number, 1 to 54, of the card (or joker) they want flipped. Every week, one and only one ticket is drawn and the card—one of 54—is taped to a board with the corresponding number flipped over. If it’s the queen of hearts, the owner of the ticket wins the pot. If not, the money stays in the pot and the process starts all over again the next week [with the remaining unflipped cards].\nSource: Business Journal Daily\n\n\nNotation and Assumptions\nSuppose there are \\(k\\) unflipped cards remaining on the board. Write \\(m = 54 - k\\) for the number of weeks that the game has been played without a winner. Suppose you buy \\(n\\) tickets to play the game. Suppose there are \\(N\\) tickets bought by other people also playing the game. Let \\(J\\) denote the current running jackpot carried over from last week (before any new tickets were bought this week).\nWe assume that the deck of cards was well-shuffled before being placed on the board. We also assume that the pot of tickets was well-mixed before selection of the one potential winning ticket.\nFinally, we suppose that nobody has x-ray vision.\n\n\nLocation of the Queen of Hearts\nAfter \\(m\\) weeks of lost games, the Queen of Hearts is equally likely to be located at any one of the remaining \\(k\\) cards. To see why, let \\(X\\) denote the random location of the Queen of Hearts. When \\(k = 54\\), then \\(\\Pr(X=x) = 1/54\\), \\(x = 1,2,\\ldots,54\\) since the deck was well-shuffled. Suppose in week one the chosen ticket was \\(x_{1}\\), and suppose that ticket was a loser. Given the information \\(X \\neq x_{1}\\), the conditional distribution of \\(X\\) is \\[\n\\Pr(X = x,\\vert,X\\neq x_{1}) = \\frac{\\Pr(X = x)}{\\Pr(X \\neq x_{1})} = \\frac{1/54}{53/54} = \\frac{1}{53},\n\\]\nfor \\(x = 1,2,\\ldots,54\\) and \\(x \\neq x_{1}\\). Continuing in this fashion, \\[\n\\Pr(X = x,\\vert,X\\neq x_{1},\\ldots,x_{m}) = \\frac{\\Pr(X = x)}{\\Pr(X \\neq x_{1},\\ldots,x_{m})} = \\frac{1/54}{(54 - m)/54} = \\frac{1}{k},\n\\] for \\(x = 1,2,\\ldots,54\\) and \\(x \\neq x_{1},\\ldots,x_{m}\\).\n\n\nProbability that somebody wins this week\nThere is a 100% chance that somebody’s ticket will be selected. That ticket will have a number on it, and that number will match the random location of the Queen of Hearts a proportion \\(1/k\\) of the time\\(\\ldots\\) no x-ray vision, remember? That means \\[\n\\Pr(\\text{Somebody wins}) = \\Pr(\\text{Number matches}) = \\frac{1}{k}.\\\n\\]\n\n\nProbability that you win this week\nThere are \\(N + n\\) total tickets in the pot, \\(n\\) of which with your name on it. Assuming a well-mixed pot, the probability that one of your tickets is selected is \\[\n\\Pr(\\text{Your ticket selected}) = \\frac{n}{N + n}.\\\n\\]\nGiven that your ticket is selected, you have probability \\(\\Pr(\\text{Number matches}) = 1/k\\) that your number matches. Therefore, the probability that you win is \\[\n\\Pr(\\text{You win}) = \\Pr(\\text{Your ticket selected} \\cap \\text{Number matches}) = \\frac{n}{k(N + n)}.\n\\]\n\n\nOdds against your winning\nIf we write \\(p = \\frac{n}{N + n} \\cdot \\frac{1}{k}\\) then \\[\n\\text{Odds against you} = \\frac{1 - p}{p} = \\frac{kN + (k - 1)n}{n}.\n\\]\n\n\nProbability that the game lasts this long\nThe game ends exactly when somebody wins, and we saw earlier that when there are \\(k\\) cards left, the probability that somebody wins is \\(1/k\\). Let \\(L\\) denote the length of the game. Then \\(L\\) can be any number \\(1,2,\\ldots,54\\). But \\(\\Pr(L = 1)\\) is \\[\n\\Pr(L = 1) = \\Pr( \\text{Somebody wins when }k\\text{ is 54}) = \\frac{1}{54}.\n\\]\nNow, in order for \\(L = 2\\) it must be that nobody wins the first week (with probability \\(53/54\\)), and then somebody wins the second week (with probability \\(1/53\\)). Thus \\[\n\\Pr(L = 2) = \\frac{53}{54}\\cdot \\frac{1}{53} = \\frac{1}{54}.\n\\]\nBy the same reasoning, \\[\n\\Pr(L = 3) = \\frac{53}{54}\\cdot \\frac{52}{53} \\cdot \\frac{1}{52} = \\frac{1}{54},\n\\] and so forth. In other words, the outcomes \\(L = 1,2,3,\\ldots,54\\) are equally likely.\nWith the above in mind, we may ask questions like, “What are the chances the game will continue until there are at most 7 cards left?” This is \\[\n\\Pr(L \\geq 48) = \\frac{7}{54} \\approx 0.1296296,\n\\] or almost a 13% chance.\n\n\nIt lasted this long, how much longer?\nGiven that the game has lasted \\(m\\) weeks, the location of the Queen of Hearts is uniformly distributed on the remaining \\(k\\) cards, so the same argument above carries through except now our deck has only \\(k\\) cards in it. For instance, given that there are \\(k = 7\\) cards left, then the probability that the game keeps going until there are at most 2 cards remaining is \\(2/7 \\approx 0.2857143\\).\n\n\nExpected winnings\nSuppose you only started playing the game this week. Denote your winnings by \\(W\\), where we understand that if you lose then your winnings are negative. You spent \\(n\\) dollars on tickets, but you will have \\(\\$(J + N + n)\\) if you win. Your expected winnings are \\[\n\\mathbb{E}(W) = -n \\Pr(\\text{Lose}) + (J + N + n)\\Pr(\\text{Win}),\n\\] and after plugging in the formulas from above we get \\[\n\\mathbb{E}(W) = -n \\left( 1 - \\frac{n}{k(N + n)}\\right) + (J + N + n)\\left( \\frac{n}{k(N + n)} \\right).\n\\]\n\n\nBut what if I played earlier weeks, too?\nLet’s say you bought \\(n^{\\ast}\\) tickets prior to this week’s game. Then after your \\(n\\) tickets this week you’ve invested \\(n + n^{\\ast}\\) total dollars in the game which you stand to lose. The \\(n^{\\ast}\\) is already contained in the \\(J\\) dollars above, because that money carries through from week to week. Your probability of winning this week doesn’t change. Your (smaller) expected winnings are \\[\n\\mathbb{E}(W) = -(n + n^{\\ast}) \\left( 1 - \\frac{n}{k(N + n)}\\right) + (J + N + n)\\left( \\frac{n}{k(N + n)} \\right).\n\\]\n\n\nExpected winnings per dollar\nJust divide \\(\\mathbb{E}(W)\\) by \\(n\\), or \\((n + n^{\\ast})\\), whatever your case may be.\n\n\nCan I try it out?\nSure. Here is an RStudio shiny app that calculates (almost) all of the above for assorted values of \\(k\\), \\(n\\), \\(N\\), and \\(J\\).\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/CitationBibTeX citation:@online{jay kerns2024,\n  author = {Jay Kerns, G.},\n  title = {The {Queen} of {Hearts}},\n  date = {2024-03-06},\n  url = {https://gjkerns.github.io/posts/2024-03-06-queen-of-hearts},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nJay Kerns, G. 2024. “The Queen of Hearts.” March 6, 2024.\nhttps://gjkerns.github.io/posts/2024-03-06-queen-of-hearts."
  },
  {
    "objectID": "BayesianDemo.html",
    "href": "BayesianDemo.html",
    "title": "Bayesian Statistics Demo",
    "section": "",
    "text": "This notebook is meant to be a quick demo of some of the main concepts of Bayesian statistics and some of the primary statistical methods that a Bayesian statistician uses. The discussion follows Chapter 2 of Richard McElreath’s book Statistical Rethinking 2nd Ed."
  },
  {
    "objectID": "BayesianDemo.html#environment-preliminaries",
    "href": "BayesianDemo.html#environment-preliminaries",
    "title": "Bayesian Statistics Demo",
    "section": "Environment preliminaries",
    "text": "Environment preliminaries\nYou will need many R packages to do Bayesian statistics. Below are commands to install a slim version of the rethinking package that will work for the purposes of this demo but we will need to do some other work to do hardcore Bayesian stuff later.\nThe commands below are for a slim version of the rethinking package for your personal laptop or desktop computer. It looks like the Colab R runtime comes with all dependencies out of the box but you will still need to install the rethinking package (the 2nd command below) before this notebook will run on Google Colab. The good news is the installation only takes a couple of minutes to complete. Note you will need to reinstall the rethinking package repeatedly for each runtime.\n\n# This line will be required to install on a personal laptop\n# install.packages(c(\"coda\",\"mvtnorm\",\"devtools\",\"loo\",\"dagitty\"))\n\n\n# need this one for Google Colab\ndevtools::install_github(\"rmcelreath/rethinking@slim\")"
  },
  {
    "objectID": "BayesianDemo.html#one-parameter-problem",
    "href": "BayesianDemo.html#one-parameter-problem",
    "title": "Bayesian Statistics Demo",
    "section": "One-parameter problem",
    "text": "One-parameter problem\nWe are going to focus on a one parameter problem and look at several approaches to attack the problem. In the real world you will have multiple parameters and your models will be more complicated but the techniques will be the same—just more extended.\nParameter of interest: \\[\np = \\text{proportion of Earth covered by water}\n\\]\nOur random experiment will be to toss a physical model Earth globe and see whether it hits the ground on an area of water (W) or land (L). The idea is that if Earth is covered by proportion \\(p\\) of water then we would expect the globe to land on water approximately \\(p\\) proportion of the time in the long run. If \\(\\text{water}=\\text{Success}\\), let \\[\nW = \\#\\text{ successes in $n$ trials}\n\\] Then \\(W\\) has what is called a binomial distribution which we denote by \\(W\\sim\\text{binom(size = $n$, prob = $p$)}\\) with \\[\n\\Pr(W=w) = \\binom{n}{w} p^{w}(1 - p)^{n-w},\\ w=0,1,2,\\ldots,n.\n\\]\nThe R command to evaluate the binomial distribution is:\ndbinom(w, size = n, prob = p)"
  },
  {
    "objectID": "BayesianDemo.html#prior-distributions-and-bayes-rule",
    "href": "BayesianDemo.html#prior-distributions-and-bayes-rule",
    "title": "Bayesian Statistics Demo",
    "section": "Prior distributions and Bayes’ Rule",
    "text": "Prior distributions and Bayes’ Rule\nWe know there is some water on the Earth (\\(p &gt; 0\\)) and we know that the polar icecaps haven’t melted yet (\\(p &lt; 1\\)), so that is, we know that \\(0 &lt; p &lt; 1\\). A valid prior is any probability distribution on \\([0,1]\\) or \\((0,1)\\) that reflects our personal beliefs regarding the value of \\(p\\) before having observed any data.\nWe next perform our random experiment to collect data. Then we use Bayes’ Rule to update our prior distribution to our posterior distribution, that is, our updated beliefs about \\(p\\) after having observed the data. Bayes’ Rule looks like this in shorthand: \\[\n\\text{Posterior} \\propto \\text{Prior} \\times \\text{Likelihood}\n\\]\nThere are four (4) main ways to compute posteriors:\n\nMathematics/Calculus (analytical)\nGrid approximation\nQuadratic approximation\nMarkov Chain Monte Carlo\n\nExample. Suppose we do the globe experiment, and from tossing the globe 9 times we observe the data WLWWWLWLW, that is, six times it landed on W and 3 times it landed on L. Therefore \\(n = 9\\) and \\(w = 6\\)."
  },
  {
    "objectID": "BayesianDemo.html#grid-approximation",
    "href": "BayesianDemo.html#grid-approximation",
    "title": "Bayesian Statistics Demo",
    "section": "Grid approximation",
    "text": "Grid approximation\nThe idea is we set up a grid of values for \\(p\\) on the range \\([0,1]\\), and then specify what we want the prior to be on those grid values. Higher prior probability on a particular value of \\(p\\) means we have higher prior belief that \\(p\\) is that value.\nThen we observe data and use Bayes’ Rule to compute our posterior distribution on those grid values. The code for it is pretty easy.\n\nLikelihood of \\(p\\): given by the code \\(\\text{dbinom(6, size = 9, prob = $p$)}\\)\n\nthe likelihood is a function of \\(p\\)\n\nPrior: Here we choose 20 equally spaced grid points.\n\nmake \\(p\\) equally likely\nexperiment with different priors (we will look at 3 different priors)\n\n\nAdvantages of grid approximation are that you can use pretty much any prior distribution that you can imagine and the output is relatively easy to understand. Some drawbacks are that the method doesn’t extend easily to multi-parameter problems and the more complicated models can be computationally intensive, sometimes prohibitively so.\n\nUniform prior\nLet’s start with a uniform prior. This takes all values of \\(p\\) to be equally likely. Since we are using 20 grid points, each possible value of \\(p\\) with have prior probability of \\(1/20\\).\n\n# define grid from 0 to 1\np_grid &lt;- seq( from=0 , to=1 , length.out=20 )\n\n# define prior, all equally likely\nprior &lt;- rep( 1 , 20 )\n\n# compute likelihood at each value in grid\nlikelihood &lt;- dbinom( 6 , size=9 , prob=p_grid )\n\n# compute product of likelihood and prior\nunstd.posterior &lt;- likelihood * prior\n\n# standardize the posterior, so it sums to 1\nposterior &lt;- unstd.posterior / sum(unstd.posterior)\n\nNow we will make a plot of our prior distribution.\n\nplot( p_grid , prior/sum(prior) , type=\"b\" ,\n    xlab=\"probability of water\" , ylab=\"prior probability\" ,\n    main=\"Uniform prior\")\nmtext( \"20 grid points\" )\n\nWe see that our prior distribution is uniformly flat at the value 0.05. Next, we will make a plot of our posterior distribution after observing water \\(w = 6\\) times after tossing the globe \\(n = 9\\) times.\n\nplot( p_grid , posterior , type=\"b\" ,\n    xlab=\"probability of water\" , ylab=\"posterior probability\",\n    main = \"Posterior with w=6 and n=9, Uniform prior\")\nmtext( \"20 grid points\" )\n\n\n\nTruncated uniform prior\nLet’s try a different prior, one that is truncated to put only prior probability on values of \\(p \\geq 0.5\\).\n\n# define truncated prior\nprior &lt;- ifelse( p_grid &lt; 0.5 , 0 , 1 )\n\n# compute likelihood at each value in grid\nlikelihood &lt;- dbinom( 6 , size=9 , prob=p_grid )\n\n# compute product of likelihood and prior\nunstd.posterior &lt;- likelihood * prior\n\n# standardize the posterior, so it sums to 1\nposterior &lt;- unstd.posterior / sum(unstd.posterior)\n\nNow let’s plot the truncated prior.\n\nplot( p_grid , prior/sum(prior) , type=\"b\" ,\n    xlab=\"probability of water\" , ylab=\"prior probability\",\n    main=\"Truncated uniform prior\")\nmtext( \"20 grid points\" )\n\nNow we will plot the posterior, which we notice is also truncated.\n\nplot( p_grid , posterior , type=\"b\" ,\n    xlab=\"probability of water\" , ylab=\"posterior probability\",\n    main = \"Posterior with w=6 and n=9, from truncated prior\" )\nmtext( \"20 points\" )\n\n\n\nDouble exponential prior\nNext let’s try a “double exponential” prior, which tails off on either side of \\(p = 0.5\\).\n\n# define prior\nprior &lt;- exp( -5*abs( p_grid - 0.5 ) )\n\n# compute likelihood at each value in grid\nlikelihood &lt;- dbinom( 6 , size=9 , prob=p_grid )\n\n# compute product of likelihood and prior\nunstd.posterior &lt;- likelihood * prior\n\n# standardize the posterior, so it sums to 1\nposterior &lt;- unstd.posterior / sum(unstd.posterior)\n\nNext we plot the double exponential prior\n\nplot( p_grid , prior/sum(prior) , type=\"b\" ,\n    xlab=\"probability of water\" , ylab=\"prior probability\",\n    main=\"Double exponential prior\")\nmtext( \"20 grid points\" )\n\nAnd let’s take a look at the new posterior.\n\nplot( p_grid , posterior , type=\"b\" ,\n    xlab=\"probability of water\" , ylab=\"posterior probability\",\n    main = \"Posterior with w=6 and n=9, double exponential prior\")\nmtext( \"20 points\" )"
  },
  {
    "objectID": "BayesianDemo.html#quadratic-approximation",
    "href": "BayesianDemo.html#quadratic-approximation",
    "title": "Bayesian Statistics Demo",
    "section": "Quadratic approximation",
    "text": "Quadratic approximation\nGrid approximation is okay, but there are fancier ways to find posteriors. A better way is to approximate the posterior with a closely fitting bell curve. The idea: as \\(n \\to \\infty\\), the Posterior will converge \\(\\to\\) normal distribution: \\[\n\\text{Posterior} \\to \\mathrm{e}^{-(p - \\mu)^2/\\sigma^2}\n\\] Then \\[\n\\ln (\\text{Posterior}) \\to -\\frac{(p - \\mu)^2}{2\\sigma^2},\\ (\\text{a parabola})\n\\] Next we use the computer to find the best fitting parabola, then estimate and report \\(\\mu\\) and \\(\\sigma\\). This procedure is called MAP, for Maximum a Posteriori.\n\nHow to do it with R\n\nlibrary(rethinking)\nglobe.qa &lt;- quap(\n    alist(\n        W ~ dbinom( W+L, p) ,  # binomial likelihood\n        p ~ dunif(0,1)         # uniform prior\n    ) ,\n    data=list(W=6,L=3) )\n\nWe stored the results of our computation in an object named globe.qa. We summarize the object with the precis() function.\n\n# display summary of quadratic approximation\nprecis( globe.qa )\n\nThe above output is saying that we can approximate the posterior in this problem with a normal distribution (bell curve) that has a mean of approximately 0.66 and a standard deviation approximately 0.16. There is a whole lot more we can do with this output and a whole bunch more stored in the globe.qa object that we haven’t discussed yet, but for many problems these kinds of computations are good enough and it is a whole lot quicker/easier than any of the other methods we will encounter.\nUnfortunately, quadratic approximation is a bit limited to well-behaved models, and well-behaved data. For some of the more complicated things you will probably be working on quadratic approximation is likely not going to be good enough—we’ll see."
  },
  {
    "objectID": "BayesianDemo.html#analytical-posteriors",
    "href": "BayesianDemo.html#analytical-posteriors",
    "title": "Bayesian Statistics Demo",
    "section": "Analytical posteriors",
    "text": "Analytical posteriors\nWe can also find posterior distributions with mathematical methods, differential and integral calculus, and the like. Here is what that looks like for this simple one parameter problem and a uniform prior. The prior distribution \\(\\pi(p)\\) has the formula \\[\n\\pi(p) = 1,\\ 0 &lt; p &lt; 1. \\quad \\text{PRIOR}\n\\] And the likelihood function \\(f(w\\vert p)\\) takes the mathematical form \\[\nf(w\\vert p) = \\binom{n}{w} p^{w}(1 - p)^{n-w},\\quad \\text{LIKELIHOOD}\n\\] Then our Posterior distribution \\(\\pi(p\\vert w)\\) is computed by means of Bayes’ Rule like this: \\[\n\\begin{aligned}\n\\pi(p\\vert w)\\propto & \\ 1\\cdot\\binom{n}{w}p^{w}(1-p)^{n-w},\\\\\n\\propto & \\ p^{(w+1)-1}(1-p)^{(n-w+1)-1},\\\\\n= & \\ \\text{Beta(df1 = $w+1$, df2 = $n-w+1$)}\n\\end{aligned}\n\\]\nThis so-called “Beta distribution” is a famous and popular family of models that mathematicians find convenient to get analytical solutions to Bayesian problems.\n\nHow to do it with R\n\n# analytical solution plotted out\nW &lt;- 6\nL &lt;- 3\ncurve( dbeta( x , W+1 , L+1 ) , from=0 , to=1, main = \"Analytical Beta Posterior when w=6, n=9 with uniform prior\" )\n# quadratic approximation\ncurve( dnorm( x , 0.67 , 0.16 ) , lty=2 , add=TRUE )\nmtext( \"dotted line is normal approximation\" )"
  },
  {
    "objectID": "BayesianDemo.html#markov-chain-monte-carlo-methods",
    "href": "BayesianDemo.html#markov-chain-monte-carlo-methods",
    "title": "Bayesian Statistics Demo",
    "section": "Markov Chain Monte Carlo methods",
    "text": "Markov Chain Monte Carlo methods\nFor the hardcore stuff there will not be any analytical solutions available to us and we will need more powerful tools than grid approximation. Furthermore, in hard problems, the normal approximation doesn’t fit very well. Enter Markov Chain Monte Carlo (MCMC) methods to the rescue. This topic is covered in Chapter 9 of Statistical Rethinking. The algorithm used below is known as the Metropolis Algorithm.\n\n# Metropolis algorithm example\n# basic accept-reject process\nn_samples &lt;- 1000\np &lt;- rep( NA , n_samples )\np[1] &lt;- 0.5\nW &lt;- 6\nL &lt;- 3\nfor ( i in 2:n_samples ) {\n    p_new &lt;- rnorm( 1 , p[i-1] , 0.1 )\n    if ( p_new &lt; 0 ) p_new &lt;- abs( p_new )\n    if ( p_new &gt; 1 ) p_new &lt;- 2 - p_new\n    q0 &lt;- dbinom( W , W+L , p[i-1] )\n    q1 &lt;- dbinom( W , W+L , p_new )\n    p[i] &lt;- ifelse( runif(1) &lt; q1/q0 , p_new , p[i-1] )\n}\n\n## next we plot a density estimate and the theoretical curve on the same graph\ndens( p , xlim=c(0,1) ,main = \"Posterior with w=6, n=9, uniform prior\")\ncurve( dbeta( x , W+1 , L+1 ) , lty=2 , add=TRUE )\nmtext( \"dotted line is true beta posterior\" )\n\nLook how the wiggly density curve approximates the actual, true Beta posterior shown by the dotted line. The approximation isn’t very good because the MCMC sampler only used n_samples = 1000.\nYou can rerun the code above yourself for a bunch of different values of n_samples to see how the density approximation gets better and better for larger values of n_samples."
  },
  {
    "objectID": "BayesianDemo.html#hmc-methods-and-beyond",
    "href": "BayesianDemo.html#hmc-methods-and-beyond",
    "title": "Bayesian Statistics Demo",
    "section": "HMC methods and beyond…",
    "text": "HMC methods and beyond…\nThe MCMC methods described above have been around for some decades and in recent years there has been some significant progress and major speed-ups to the methods, but this means even more parameters and requirements for even higher performance computing. The latest trend in Bayesian statistics deals with Hamiltonian Monte Carlo (HMC) methods and a programming interface called Stan. See here for more:\nhttps://mc-stan.org/\nStan interfaces with the most popular data analysis languages (R, Python, shell, MATLAB, Julia, Stata) and runs on all major platforms (Linux, Mac, and Windows). We very well may need Stan before it’s all said and done."
  },
  {
    "objectID": "BayesianDemo.html#bibliography",
    "href": "BayesianDemo.html#bibliography",
    "title": "Bayesian Statistics Demo",
    "section": "Bibliography",
    "text": "Bibliography\nMcElreath, R. (2018). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman and Hall/CRC."
  }
]